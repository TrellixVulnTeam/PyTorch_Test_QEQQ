

# reference: https://pytorch.apachecn.org/docs/1.7/27.html

# nn.Transformer模块完全依赖于注意力机制(另一个最近实现为nn.MultiheadAttention的模块)来绘制输入和输出之间的
# 全局依存关系. nn.Transformer模块现已高度模块化，因此可以轻松地修改/组成单个组件.
